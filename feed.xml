<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://shunk031.github.io/paper-survey/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shunk031.github.io/paper-survey/" rel="alternate" type="text/html" /><updated>2019-08-24T08:17:22+00:00</updated><id>https://shunk031.github.io/paper-survey/feed.xml</id><title type="html">Paper Survey</title><subtitle>Survey of previous research and related works on machine learning (especially Deep Learning) in Japanese
</subtitle><entry><title type="html">Personalized Fashion Recommendation with Visual Explanations based on Multimodal Attention-Network</title><link href="https://shunk031.github.io/paper-survey/summary/others/Personalized-Fashion-Recommendation-with-Visual-Explanations-based-on-Multimodal-Attention-Network" rel="alternate" type="text/html" title="Personalized Fashion Recommendation with Visual Explanations based on Multimodal Attention-Network" /><published>2019-08-22T00:00:00+00:00</published><updated>2019-08-22T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Personalized-Fashion-Recommendation-with-Visual-Explanations-based-on-Multimodal-Attention-Network</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Personalized-Fashion-Recommendation-with-Visual-Explanations-based-on-Multimodal-Attention-Network">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;ユーザー単位でファッション画像・レビューテキストを用いた解釈性のある注意機構つきファッション商品推薦システムを提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;ファッションアイテムのオンラインショッピングがますます盛んになっており、ファッション業界や学術界ではファッションに対する推薦が注目を集めている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Personalized-Fashion-Recommendation-with-Visual-Explanations-based-on-Multimodal-Attention-Network/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ファッション商品には画像の他に商品のレビューが付与されている。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ユーザ A は首元にフォーカスしている&lt;/li&gt;
  &lt;li&gt;ユーザ B は服のポケットにフォーカスしている&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ファッション画像全体の特徴を捉えるのではなく、ユーザーごといくつかの領域にフォーカスする必要がある。&lt;/p&gt;

&lt;p&gt;先行研究ではファッション商品の推薦を行うモデルにおいて、以下の 3 つの点で疑問点が残る:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ファッション画像全体の特徴を捉えるのではなく、ユーザごとにいくつかのサブ領域に着目すべき&lt;/li&gt;
  &lt;li&gt;複数の要素が組み合わさったファッション画像をそのままエンコードするのは、モデルの学習時にノイズとなりうる&lt;/li&gt;
  &lt;li&gt;ユーザの勾配体験向上のためには、推薦の根拠の提示や説明性の向上が重要である&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;これら 3 つの疑問点を解決し、より効果的なファッション推薦システムを行うモデルが必要である。
しかしこれらの疑問点を解決するには、以下の問題点が存在する:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;情報量の少ない教師信号
    &lt;ul&gt;
      &lt;li&gt;先行研究ではクリック等のスパースで暗黙的なフィードバックを教師信号として使用している&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;個々人に合った商品画像の興味領域の選択
    &lt;ul&gt;
      &lt;li&gt;個々人の興味を元にファッション画像に対してアノテーションするのは時間がかかり、そもそも教師ラベルを定義するのも難しい&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;評価データセットの欠如
    &lt;ul&gt;
      &lt;li&gt;特に説明性のある可視化結果に対する評価に利用できるデータセットは非常に少ない&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本研究ではこれらの問題に対処するため、Visually Explanable Collaborative Filtering (VECF) を提案している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Personalized-Fashion-Recommendation-with-Visual-Explanations-based-on-Multimodal-Attention-Network/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fine-grained-visual-preference-modeling&quot;&gt;Fine-grained Visual Preference Modeling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ファッション画像に対して事前学習済み VGG19 の &lt;code class=&quot;highlighter-rouge&quot;&gt;conv5&lt;/code&gt; ブロック (最終ブロック) を画像特徴量として利用
    &lt;ul&gt;
      &lt;li&gt;計算リソースが許すなら fine-tuning してもよいが今回は実施していない&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ユーザ単位の fine-grained な視覚特徴を考慮するために visual attention 機構を導入
    &lt;ul&gt;
      &lt;li&gt;推薦時に視覚的な説明性を提供することも可能&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;review-enhanced-model-supervision&quot;&gt;Review enhanced Model Supervision&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ファッション商品のレビューテキストにはユーザの趣向が含まれている
    &lt;ul&gt;
      &lt;li&gt;クリック等のスパースで暗黙的なフィードバックと比較すると、よりリッチな情報を含んでいる&lt;/li&gt;
      &lt;li&gt;弱教師 (weak supervision signal) としてレビュー情報を使用し、推薦のパフォーマンスや説明性を向上させる&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LSTM をベースに、抽出した画像特徴を concat して学習させる
    &lt;ul&gt;
      &lt;li&gt;論文中の図では GRU と書いてあるが本文では LSTM を使用していると記述あり&lt;/li&gt;
      &lt;li&gt;このコンポーネントは推薦スコアの予測には直接必要ないため、より計算量的な観点で実践的である&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ハイパーパラメータ &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; によって暗黙的なフィードバックかレビューテキストのトレードオフを決める。&lt;/p&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;p&gt;ファッションデータセットとして Amazon.com をベースとした公開データセットを使用し、ベースラインの手法と提案手法を比較している。またレビューテキストを考慮しないモデル VECF(-rev)と visual attention を適用しない VECF(-att)を比較対象に追加している。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;p&gt;ファッション推薦システムにおける視覚的な説明性を評価するのは著者らが知る限り本研究が初であるため、評価できるデータセットもまた存在しない。&lt;/p&gt;

&lt;p&gt;クラウドソーシングを使ってこれらを評価できるデータセットを構築した。
商品レビューからユーザが 7x7 の格子に分けたファッション画像のどの領域に着目しているかをワーカーがラベリングした。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;定量的評価
    &lt;ul&gt;
      &lt;li&gt;VECF と VECF(-rev)に対して視覚的説明性を比較&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;定量的評価
    &lt;ul&gt;
      &lt;li&gt;VECF と VECF(-rev)における attention の可視化を定性的に評価&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Personalized-Fashion-Recommendation-with-Visual-Explanations-based-on-Multimodal-Attention-Network/figure3.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;p&gt;説明性のあるレコメンデーションについて&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;HFT
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2507163&quot;&gt;McAuley, Julian, and Jure Leskovec. “Hidden factors and hidden topics: understanding rating dimensions with review text.” Proceedings of the 7th ACM conference on Recommender systems. ACM, 2013.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RBLT
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.ijcai.org/Proceedings/16/Papers/375.pdf&quot;&gt;Tan, Yunzhi, et al. “Rating-boosted latent topics: Understanding users and items with ratings and reviews.” IJCAI. Vol. 16. 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;D-attn
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3109890&quot;&gt;Seo, Sungyong, et al. “Interpretable convolutional neural networks with dual local and global attention for review rating prediction.” Proceedings of the Eleventh ACM Conference on Recommender Systems. ACM, 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NARRE
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3186070&quot;&gt;Chen, Chong, et al. “Neural attentional rating regression with review-level explanations.” Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee, 2018.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CARL
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3298988&quot;&gt;Wu, Libing, et al. “A context-aware user-item representation learning for item recommendation.” ACM Transactions on Information Systems (TOIS) 37.2 (2019): 22.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MPCN
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3220086&quot;&gt;Tay, Yi, Anh Tuan Luu, and Siu Cheung Hui. “Multi-pointer co-attention networks for recommendation.” Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining. ACM, 2018.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DER
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.aaai.org/ojs/index.php/AAAI/article/view/3768&quot;&gt;Chen, Xu, Yongfeng Zhang, and Zheng Qin. “Dynamic Explainable Recommendation based on Neural Attentive Models.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NRT
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3080822&quot;&gt;Li, Piji, et al. “Neural rating regression with abstractive tips generation for recommendation.” Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;gC2S
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.09900&quot;&gt;Tang, Jian, et al. “Context-aware natural language generation with recurrent neural networks.” arXiv preprint arXiv:1611.09900 (2016).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NOR
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8669792/&quot;&gt;Lin, Yujie, et al. “Explainable Outfit Recommendation with Joint Outfit Matching and Comment Generation.” IEEE Transactions on Knowledge and Data Engineering (2019).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;KSR
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3210017&quot;&gt;Huang, Jin, et al. “Improving sequential recommendation with knowledge-enhanced memory networks.” The 41st International ACM SIGIR Conference on Research &amp;amp; Development in Information Retrieval. ACM, 2018.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ECFKG
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.mdpi.com/1999-4893/11/9/137&quot;&gt;Ai, Qingyao, et al. “Learning heterogeneous knowledge base embeddings for explainable recommendation.” Algorithms 11.9 (2018): 137.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RippleNet
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3271739&quot;&gt;Wang, Hongwei, et al. “Ripplenet: Propagating user preferences on the knowledge graph for recommender systems.” Proceedings of the 27th ACM International Conference on Information and Knowledge Management. ACM, 2018.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;KPRN
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.aaai.org/ojs/index.php/AAAI/article/view/4470&quot;&gt;Wang, Xiang, et al. “Explainable reasoning over knowledge graphs for recommendation.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;KTUP
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3313705&quot;&gt;Cao, Yixin, et al. “Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences.” The World Wide Web Conference. ACM, 2019.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3331254&quot;&gt;Chen, Xu, et al. “Personalized Fashion Recommendation with Visual Explanations based on Multimodal Attention Network: Towards Visually Explainable Recommendation.” Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2019.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Putting Fairness Principles into Practice Challenges Metrics and Improvements</title><link href="https://shunk031.github.io/paper-survey/summary/others/Putting-Fairness-Principles-into-Practice-Challenges-Metrics-and-Improvements" rel="alternate" type="text/html" title="Putting Fairness Principles into Practice Challenges Metrics and Improvements" /><published>2019-08-14T00:00:00+00:00</published><updated>2019-08-14T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Putting-Fairness-Principles-into-Practice-Challenges-Metrics-and-Improvements</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Putting-Fairness-Principles-into-Practice-Challenges-Metrics-and-Improvements">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;機械学習における公正性に向けて、プロダクション環境へ適用可能でグループ間のバイアスに頑健となる absolute correlation regularization を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;ユーザの特徴を用いた学習データには偏りが存在する場合があり、こうした偏りは公正な予測の妨げとなる。
特に機械学習システムをプロダクション環境で運用する際には、公正な予測を提供する必要がある。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Putting-Fairness-Principles-into-Practice-Challenges-Metrics-and-Improvements/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;あるデータセットにおいて従来手法で予測した場合、センシティブな属性で分かれるあるグループとそれ以外のグループの間に予測のバイアスが観測される例がある (左図)。
本来なら右図のようにグループ間で予測の差は現れないはずである。
こうした予測のバイアスを解消するため、Adversarial training を用いたグループ間の偏りに頑健な学習が提案されているが、学習時に不安定であったり、そもそも学習が難しい場合が多い。&lt;/p&gt;

&lt;p&gt;本研究ではあるグループにおける公正性、特にグループ間の &lt;code class=&quot;highlighter-rouge&quot;&gt;False Positive&lt;/code&gt; の比較に焦点を当て、こうした false positive を抑制する新たな正則化手法である absolute correlation regularization を提案している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;Deep Neural Network (DNN) に通常の損失関数に加えて &lt;code class=&quot;highlighter-rouge&quot;&gt;absolute correlation regularization&lt;/code&gt; を導入。&lt;/p&gt;

&lt;h3 id=&quot;absolute-correlation-regularization&quot;&gt;Absolute correlation regularization&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{f} \left[\sum_{(\mathbf{x}_i,y_i) \in \mathcal{X}}^{} \textrm{L}(y_i, f(\mathbf{x}_i))\right] + \lambda |Corr_{\mathcal{X}^{-}}|&lt;/script&gt;

&lt;p&gt;ここで &lt;script type=&quot;math/tex&quot;&gt;Corr_{\mathcal{X}^{-}}&lt;/script&gt; は予測値 &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt; とセンシティブな属性 &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; との相対的な相関を最小化する:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Corr_{\mathcal{X}^{-}} = \frac{(\sum_{\mathbf{X}_i \in \mathcal{X}^{-}}^{} f(\mathbf{x}_i) - \mu_{\hat{y}}) (\sum_{s_i \in \mathcal{X}^{-}}^{} s_i - \mu_s)}{\sigma_{\hat{y}} \sigma_s}&lt;/script&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;線形モデル、DNN、DNN w/ adversarial training、DNN w/ absolute correlation regularization の 4 つのモデルを適用した結果を比較している。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Putting-Fairness-Principles-into-Practice-Challenges-Metrics-and-Improvements/figure3-1.png&quot; alt=&quot;Figure 3-1&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Putting-Fairness-Principles-into-Practice-Challenges-Metrics-and-Improvements/figure3-2.png&quot; alt=&quot;Figure 3-2&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Putting-Fairness-Principles-into-Practice-Challenges-Metrics-and-Improvements/figure3-3.png&quot; alt=&quot;Figure 3-3&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Putting-Fairness-Principles-into-Practice-Challenges-Metrics-and-Improvements/figure3-4.png&quot; alt=&quot;Figure 3-4&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;比較対象のモデルに対して、absolute correlation regularization を導入した DNN の予測はグループ間で偏りが小さいことを確認した。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;p&gt;モデルの予測とセンシティブな属性との相関を無相関にすることで、結果的に予測結果のバイアスが取り除かれるように学習が進むと考えられる。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Adversarial training を用いたグループ間の偏りに頑健な学習
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.00075&quot;&gt;Beutel, Alex, et al. “Data decisions and theoretical implications when adversarially learning fair representations.” arXiv preprint arXiv:1707.00075 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.04562&quot;&gt;Beutel, Alex, et al. “Putting fairness principles into practice: Challenges, metrics, and improvements.” arXiv preprint arXiv:1901.04562 (2019).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">DropAttention: A Regularization Method for Fully Connected Self Attention-Networks</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks" rel="alternate" type="text/html" title="DropAttention: A Regularization Method for Fully Connected Self Attention-Networks" /><published>2019-07-27T00:00:00+00:00</published><updated>2019-07-27T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;全結合からなる self-attention 層に対して dropout を行う DropAttention を用いて過学習抑制し汎化性能向上を確認した。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;Dropout は全結合層に対して確率的にノード落とすことで過学習を抑えることができる。
こうした手法は recurrent neural network (RNN) や convolutional neural network (CNN) にも応用されており、それぞれパフォーマンスの向上が確認されている。&lt;/p&gt;

&lt;p&gt;本研究では dropout 手法を全結合からなる self-attention に対して適用する DropAttention を提案し、汎化性能向上を確認している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;dropattention&quot;&gt;DropAttention&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;複数種類の dropout 手法を比較
    &lt;ul&gt;
      &lt;li&gt;DropAttention(c)
        &lt;ul&gt;
          &lt;li&gt;列ベースで dropout する。先行研究の Dropout と同様の振る舞い。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;DropAttention(e)
        &lt;ul&gt;
          &lt;li&gt;要素ベースで dropout する。先行研究の DropConnect と同賞の振る舞い。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;これらの DropAttention にはハイパーパラメータとして dropout 率 &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; と drop 対象となる連続した範囲を示すウィンドウサイズ &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; の設定が必要である。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dropout 後のリスケーリング処理
    &lt;ul&gt;
      &lt;li&gt;Attention に対して dropout した後に再度総和を取ると 1 となるようにリスケーリングを行う&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;テキスト分類、系列ラベリング、テキスト含意認識、機械翻訳の 4 つのタスクに対して、DropAttention(c)および DropAttention(e)の効果確認を行っている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;DropAttention(c)および(e)のハイパーパラメータについて
    &lt;ul&gt;
      &lt;li&gt;DropAttention(c)の場合 dropout 率は大きく、小さいウィンドウサイズを用いるのが良い&lt;/li&gt;
      &lt;li&gt;DropAttention(e)の場合 dropout 率は小さく、大きいウィンドウサイズを用いるのが良い&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;先行研究である dropout も適用後にリスケーリング処理を行うが、同様の処理を DropAttention に適用すると性能が悪くなることが確認された。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dropout 関連の論文がまとまっているので参考になる&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks/table1.png&quot; alt=&quot;Table 1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.11065&quot;&gt;Lin Zehui, Pengfei Liu, Luyao Huang, Jie Fu, Junkun Chen, Xipeng Qiu, Xuanjing Huang. DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks. arXiv:1907.11065, 2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">From Small-scale to Large-scale Text Classification</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/From-Small-scale-to-Large-scale-Text-Classification" rel="alternate" type="text/html" title="From Small-scale to Large-scale Text Classification" /><published>2019-07-09T00:00:00+00:00</published><updated>2019-07-09T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/From-Small-scale-to-Large-scale-Text-Classification</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/From-Small-scale-to-Large-scale-Text-Classification">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;カテゴリ数が非常に多いテキスト分類と比較的カテゴリ数の少ないテキスト分類を同時に行うマルチタスクなモデルアーキテクチャを提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;テキスト分類においてカテゴリ数が非常に多い &lt;code class=&quot;highlighter-rouge&quot;&gt;large-scale&lt;/code&gt; な問題設定と、比較的カテゴリ数の少ない &lt;code class=&quot;highlighter-rouge&quot;&gt;small-scale&lt;/code&gt; な問題設定がある。&lt;/p&gt;

&lt;p&gt;Web 検索のパーソナライズやレコメンドシステムなど、カテゴリ数が非常に多いテキスト分類は学習時に各カテゴリを捉えられるよう大規模なデータセットが必要である。&lt;/p&gt;

&lt;p&gt;先行研究ではカテゴリ数の多いデータに対する研究はあるが、これらの手法は単語間の意味的類似性の重要度を考慮せずに、用語に対する重み付け手法のみを考慮している。
また、十分なデータ数が確保できない場合に対してマルチタスク学習を適用する研究はあるが、カテゴリ数の非常に多い大規模なテキスト分類に対する研究はとても少ない。&lt;/p&gt;

&lt;p&gt;本研究ではカテゴリ数が非常に大きいものと比較的小さいもののテキスト分類のために、タスク間で有効な特徴を共有するマルチタスク学習と、
各タスクに有効な特徴を選択する gate 構造 を導入した convolutional neural network (CNN) ベースのネットワークを提案している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/From-Small-scale-to-Large-scale-Text-Classification/figure1.png&quot; width=&quot;600px&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;カテゴリ数の多い大規模なテキスト分類に対するマルチタスク学習&quot;&gt;カテゴリ数の多い大規模なテキスト分類に対するマルチタスク学習&lt;/h3&gt;
&lt;h4 id=&quot;shared-layer-と-private-layer&quot;&gt;Shared layer と Private layer&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;タスク間で不変な特徴とタスクで独立な特徴をそれぞれ捉えるモデルアーキテクチャ
    &lt;ul&gt;
      &lt;li&gt;Shared Layer
        &lt;ul&gt;
          &lt;li&gt;各タスクに対して共有の畳込み+プーリングを用いて、タスク間で共通の特徴を学習可能&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Private Layer
    &lt;ul&gt;
      &lt;li&gt;タスク固有の畳み込み+プーリングを用いて、タスク独立な特徴を学習可能&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;gate-構造&quot;&gt;Gate 構造&lt;/h4&gt;
&lt;p&gt;小規模なテキスト分類タスクから大規模テキスト分類タスクへ効果的な特徴を選択する。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{g} = \sigma(\textbf{W}_{\textrm{small} \to \textrm{large}} \textbf{z}_{\textrm{small}} + \textbf{b}_{\textrm{small} \to \textrm{large}})&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Large-scale&lt;/code&gt; テキスト分類タスクにおいて、ゲート &lt;script type=&quot;math/tex&quot;&gt;{\bf g}&lt;/script&gt; を用いて最終的な特徴量を得る
    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{z}'_{\textrm{large}} = \textbf{z}_{\textrm{large}} + \textbf{z}_{\textrm{share}} + \textbf{g} \odot \textbf{z}_{\textrm{small}}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Small-scale&lt;/code&gt; テキスト分類タスクにおいては以下となる
    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{z}'_{\textrm{small}} = \textbf{z}_{\textrm{small}} + \textbf{z}_{\textrm{share}}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;モデルの学習&quot;&gt;モデルの学習&lt;/h3&gt;
&lt;h4 id=&quot;joint-traning&quot;&gt;Joint Traning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;ランダムにタスクを選択する (&lt;code class=&quot;highlighter-rouge&quot;&gt;large-scale&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;small-scale&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;選択したタスクに沿った学習データを選択する&lt;/li&gt;
  &lt;li&gt;これら学習データをもとに勾配降下法でパラメータを更新する&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;oversampling&quot;&gt;Oversampling&lt;/h4&gt;
&lt;p&gt;カテゴリ数の多いデータセットの場合、カテゴリごとで分布に差があるため、少ないカテゴリをオーバーサンプリングして対処する。&lt;/p&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h3 id=&quot;データセット&quot;&gt;データセット&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Open Directory Project (ODP)
    &lt;ul&gt;
      &lt;li&gt;大規模な木構造の web ページディレクトリ
 	- 15 階層に及び、前処理後には 3000 近くのカテゴリが含まれている。
        &lt;ul&gt;
          &lt;li&gt;大分類と小分類が含まれている。
            &lt;ul&gt;
              &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Top/Sports&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Top/Health&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Top/Computers&lt;/code&gt;&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Top/Sports/Baseball/Major_League/Teams/Los_Angles_Dogders/News_and_Media&lt;/code&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;学習データとして 23,000 ページを対象
 	  - &lt;code class=&quot;highlighter-rouge&quot;&gt;Smal-scale&lt;/code&gt; テキスト分類タスク用にトップレベルの 13 カテゴリを使用&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;New York Times (NYT)
    &lt;ul&gt;
      &lt;li&gt;ODP の追加テストデータとして、new york times の記事から &lt;code class=&quot;highlighter-rouge&quot;&gt;art&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;business&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;food&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;health&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;politics&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;sports&lt;/code&gt; なカテゴリをランダムに取得して評価に使用した。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;評価指標&quot;&gt;評価指標&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;F1 score をベースに使用
    &lt;ul&gt;
      &lt;li&gt;Micro-averaging (Mi-) F1 score
        &lt;ul&gt;
          &lt;li&gt;テストデータ全体の F1 スコア
            &lt;ul&gt;
              &lt;li&gt;データ数の多いカテゴリを正確に当てられているか&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Macro-averaging (Ma-) F1 score
        &lt;ul&gt;
          &lt;li&gt;各カテゴリ数に応じた F1 スコア
 	  - データ数の少ないカテゴリを正確に当てられているか&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NYT データセットに対しては precision@k を使用
    &lt;ul&gt;
      &lt;li&gt;記事に対してアノテータが ODP のカテゴリとマッチするように&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ベースラインモデルとの比較&quot;&gt;ベースラインモデルとの比較&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ベースモデル
    &lt;ul&gt;
      &lt;li&gt;MC (Merge-Centroid)&lt;/li&gt;
      &lt;li&gt;CNN&lt;/li&gt;
      &lt;li&gt;LSTM&lt;/li&gt;
      &lt;li&gt;BiLSTM&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;単語ベクトル系
    &lt;ul&gt;
      &lt;li&gt;PV&lt;/li&gt;
      &lt;li&gt;fastText&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;マルチタスク系
    &lt;ul&gt;
      &lt;li&gt;MT-DNN&lt;/li&gt;
      &lt;li&gt;MT-CNN&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;提案手法
    &lt;ul&gt;
      &lt;li&gt;SP-LSTM
        &lt;ul&gt;
          &lt;li&gt;Share layer + Private layer w/o Gate&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;SPG-CNN
        &lt;ul&gt;
          &lt;li&gt;Share layer + Private layer w/ Gate&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h3 id=&quot;gate-構造の比較&quot;&gt;Gate 構造の比較&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;大規模分類タスクと小規模分類タスク間の gate 方向について
    &lt;ul&gt;
      &lt;li&gt;large → small 方向より、small → large 方向のほうが良かった&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;異なる事前学習済み単語ベクトルを使用したときの精度比較&quot;&gt;異なる事前学習済み単語ベクトルを使用したときの精度比較&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;word2vec や GloVe より fastText を用いたほうがよかった
    &lt;ul&gt;
      &lt;li&gt;fastText は OOV に強い&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;odp-データセットに対する精度比較&quot;&gt;ODP データセットに対する精度比較&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Large-scale&lt;/code&gt; テキスト分類タスクでは提案手法 SPG-CNN が outperform&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Small-scale&lt;/code&gt; テキスト分類タスクにおいては share layer + private layer のある LSTM が outperform
    &lt;ul&gt;
      &lt;li&gt;データが十分にあってカテゴリ間の分布が均衡なものは CNN より LSTM が強い傾向は先行研究でも確認されている&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nyt-データセットに対する精度比較&quot;&gt;NYT データセットに対する精度比較&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;提案手法の SPG-CNN が outperform
    &lt;ul&gt;
      &lt;li&gt;そのほかのマルチタスクモデルもシングルタスクモデルよりも良い結果&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;MC (Merge-Centroid) について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2529997&quot;&gt;Lee, Jung-Hyun, et al. “Semantic contextual advertising based on the open directory project.” ACM Transactions on the Web (TWEB) 7.4 (2013): 24.p&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3313563&quot;&gt;Kim, Kang-Min, et al. “From Small-scale to Large-scale Text Classification.” The World Wide Web Conference. ACM, 2019.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Content-Based Citation Recommendation</title><link href="https://shunk031.github.io/paper-survey/summary/others/Content-Based-Citation-Recommendation" rel="alternate" type="text/html" title="Content-Based Citation Recommendation" /><published>2019-06-25T00:00:00+00:00</published><updated>2019-06-25T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Content-Based-Citation-Recommendation</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Content-Based-Citation-Recommendation">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;メタデータに依存しない論文内容に基づいた、学術論文の草案に対する引用文献の推薦システムの提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;既存の引用文献レコメンドシステムは著者の名前や出版社・学会名等のメタデータに依存している場合が多かった。
こうしたメタデータはレビュー時であったり草案時には利用が難しい。&lt;/p&gt;

&lt;p&gt;本研究では既存の研究と異なり、論文内容に基づいて引用候補を識別するために使用したベクトルと同じ空間に埋め込み、
再学習が不要なモデルの構築が可能となる。
また計算の効率がよく、学習時・予測時にスケーラブルなモデルである。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Content-Based-Citation-Recommendation/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;phase-1---candidate-selection-nnselect&quot;&gt;Phase 1 - Candidate Selection (NNSelect)&lt;/h3&gt;
&lt;p&gt;学習させた文書の埋込モデルを使って、クエリ論文に近い論文を推薦候補として取得する。&lt;/p&gt;

&lt;h4 id=&quot;文書の埋め込みモデル&quot;&gt;文書の埋め込みモデル&lt;/h4&gt;
&lt;p&gt;論文のタイトルとアブストラクトを用いて文書埋め込みを計算する。
タイトルとアブストラクトをそれぞれ Bag-of-Words 形式の表現にした後に重み付け和を取ったものを使用した。
これら重み付けタイトル・アブストラクト表現に対してそれぞれ学習可能なパラメータを用いた重み付け和を計算し、文書の埋め込み表現を取得する。&lt;/p&gt;

&lt;h4 id=&quot;モデルの学習&quot;&gt;モデルの学習&lt;/h4&gt;
&lt;p&gt;クエリ論文 &lt;script type=&quot;math/tex&quot;&gt;d_q&lt;/script&gt; と引用されている論文 &lt;script type=&quot;math/tex&quot;&gt;d^{+}&lt;/script&gt; 、引用されていない文書 &lt;script type=&quot;math/tex&quot;&gt;d^{-}&lt;/script&gt; の triplet を用いて学習を行う。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\rm loss} = \max{( \alpha + s(d_q, d^{+}) - s(d_q, d^{+}), 0)}&lt;/script&gt;

&lt;p&gt;このとき &lt;script type=&quot;math/tex&quot;&gt;s(d_i, d_j)&lt;/script&gt; は文書埋め込みのコサイン類似度 &lt;script type=&quot;math/tex&quot;&gt;{\rm cos\-sim} ({\bf e}_{d_i}, {\bf e}_{d}_j_)&lt;/script&gt; として定義される。&lt;/p&gt;

&lt;h4 id=&quot;負例選択&quot;&gt;負例選択&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Random
    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;d_q&lt;/script&gt; に引用されていない論文をランダムに選択&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Negative nearest neighbors
    &lt;ul&gt;
      &lt;li&gt;埋め込み空間上で&lt;script type=&quot;math/tex&quot;&gt;d_q&lt;/script&gt;に近いが引用されていない論文を選択&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Citation-of-citation
    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;d_q&lt;/script&gt;には直接引用されていないが、&lt;script type=&quot;math/tex&quot;&gt;d_q&lt;/script&gt;が引用している論文に引用されている論文&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phase-2---reranking-candidates-nnrank&quot;&gt;Phase 2 - Reranking Candidates (NNRank)&lt;/h3&gt;
&lt;p&gt;推薦候補論文はクエリ論文に対して予測確率に基づいてソートされ、トップの論文が候補として提示する。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Content-Based-Citation-Recommendation/figure2.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;入力特徴&quot;&gt;入力特徴&lt;/h4&gt;
&lt;p&gt;提案モデルでは論文のタイトルやアブストラクトのみで、メタデータを使用せずとも先行研究を超える性能を発揮するが、より性能を向上させるために以下の特徴を考慮した。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;タイトル&lt;/li&gt;
  &lt;li&gt;アブストラクト&lt;/li&gt;
  &lt;li&gt;著者名&lt;/li&gt;
  &lt;li&gt;出版社名 (学会名)&lt;/li&gt;
  &lt;li&gt;キーワード&lt;/li&gt;
  &lt;li&gt;クエリ論文と候補論文とのテキストの交差特徴&lt;/li&gt;
  &lt;li&gt;論文被引用数&lt;/li&gt;
  &lt;li&gt;テキストの埋め込み表現のコサイン類似度&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;モデルのアーキテクチャ&quot;&gt;モデルのアーキテクチャ&lt;/h4&gt;
&lt;h4 id=&quot;モデルの学習-1&quot;&gt;モデルの学習&lt;/h4&gt;
&lt;p&gt;入力特徴を concat したものをフィードフォワードニューラルネットワークに入力する。
損失関数として&lt;code class=&quot;highlighter-rouge&quot;&gt;NNSelect&lt;/code&gt;と同様に&lt;code class=&quot;highlighter-rouge&quot;&gt;triplet loss&lt;/code&gt;を使用する。
ここでは &lt;script type=&quot;math/tex&quot;&gt;s(d_i, d_j)&lt;/script&gt; としてフィードフォワードニューラルネットワークの出力を sigmoid の通した値を使用する。
テスト時にはこの &lt;script type=&quot;math/tex&quot;&gt;s(d_i, d_j)&lt;/script&gt; の値が一番高いものを推薦候補として使用する。&lt;/p&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;p&gt;論文引用推薦の評価に用いられる DBLP、PubMed データセットを使用した。
また新たに &lt;code class=&quot;highlighter-rouge&quot;&gt;OpenCorpus&lt;/code&gt; という 700 万件程度のコンピュータサイエンスやニューロサイエンスの科学論文を集めたデータセットを構築し、モデルの評価に使用した。&lt;/p&gt;

&lt;p&gt;ベースラインとして先行研究の Cluscite と BM25 に対して提案手法の比較を行った。
評価指標として &lt;code class=&quot;highlighter-rouge&quot;&gt;Mean Reciprocal Rank (MRR)&lt;/code&gt; と &lt;code class=&quot;highlighter-rouge&quot;&gt;F1@20&lt;/code&gt; を使用した。&lt;/p&gt;

&lt;p&gt;候補論文を &lt;code class=&quot;highlighter-rouge&quot;&gt;NNSelect&lt;/code&gt; で探索するに当たり、近傍探索アルゴリズムとして &lt;code class=&quot;highlighter-rouge&quot;&gt;Annoy&lt;/code&gt; を使用した。
また提案手法のハイパーパラメータを &lt;code class=&quot;highlighter-rouge&quot;&gt;hyperopt&lt;/code&gt; で最適化した。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h3 id=&quot;負例選択について&quot;&gt;負例選択について&lt;/h3&gt;
&lt;p&gt;モデルの学習時に埋め込み空間上でクエリ論文に近いが引用されていない論文を負例として使用することで、効率的に学習することが分かった。&lt;/p&gt;

&lt;h3 id=&quot;有効な特徴について&quot;&gt;有効な特徴について&lt;/h3&gt;
&lt;p&gt;各特徴量の貢献度合いを見ると、テキストの交差特徴が他の特徴量よりも予測に貢献していることが分かった。&lt;/p&gt;

&lt;h3 id=&quot;出版社学会名に対する頑健性について&quot;&gt;出版社・学会名に対する頑健性について&lt;/h3&gt;
&lt;p&gt;出版社や学会のランクに対してロバストなモデルの構築が可能であることが分かった。&lt;/p&gt;

&lt;h3 id=&quot;テキスト特徴のエンコーディングについて&quot;&gt;テキスト特徴のエンコーディングについて&lt;/h3&gt;
&lt;p&gt;CNN や RNN をテキストのエンコーディングに使用したが、それほど性能の向上には寄与しなかった。
本研究で使用している BoW ベースの文書埋め込みを使用することで計算量を小さくスケーラブルなモデルになる。&lt;/p&gt;

&lt;h3 id=&quot;クエリ論文に対する近傍の数について&quot;&gt;クエリ論文に対する近傍の数について&lt;/h3&gt;
&lt;p&gt;実験結果から 5-近傍のときに評価指標が最大となることがわかった。&lt;/p&gt;

&lt;h3 id=&quot;自己引用のバイアスについて&quot;&gt;自己引用のバイアスについて&lt;/h3&gt;
&lt;p&gt;メタデータ（例えば著者）で訓練されたモデルは、自己引用や他の有名な著者に偏っている可能性があると仮定し、
メタデータの有無でそれぞれ &lt;code class=&quot;highlighter-rouge&quot;&gt;NNRank&lt;/code&gt; モデルを学習させる実験を行った。
メタデータを使って訓練されたモデルは、クエリ論文の著者の 1 人によって作成された論文を上位に推薦する傾向を確認した。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Cuscite について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2623630&quot;&gt;Ren, Xiang, et al. “Cluscite: Effective citation recommendation by information network-based clustering.” Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/papers/N/N18/N18-1022/&quot;&gt;Bhagavatula, Chandra, et al. “Content-Based Citation Recommendation.” Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？ メタデータに依存しない論文内容に基づいた、学術論文の草案に対する引用文献の推薦システムの提案。</summary></entry><entry><title type="html">Class-Balanced Loss Based on Effective Number of Samples</title><link href="https://shunk031.github.io/paper-survey/summary/cv/Class-Balanced-Loss-Based-on-Effective-Number-of-Samples" rel="alternate" type="text/html" title="Class-Balanced Loss Based on Effective Number of Samples" /><published>2019-05-25T00:00:00+00:00</published><updated>2019-05-25T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/cv/Class-Balanced-Loss-Based-on-Effective-Number-of-Samples</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/cv/Class-Balanced-Loss-Based-on-Effective-Number-of-Samples">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;Long-tail な不均衡データに対して、各クラス数の分布を適切に考慮した class-balanced loss を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Class-Balanced-Loss-Based-on-Effective-Number-of-Samples/figure1.png&quot; width=&quot;400px&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;実世界のデータセットは long-tail な分布を持つ不均衡データであることが多い。
こうした不均衡データに対して、先行研究では主に &lt;code class=&quot;highlighter-rouge&quot;&gt;re-sampling&lt;/code&gt; と &lt;code class=&quot;highlighter-rouge&quot;&gt;cost-sensitive learning&lt;/code&gt; の観点から解決が図られてきた。
&lt;code class=&quot;highlighter-rouge&quot;&gt;Re-sampling&lt;/code&gt; における over-sampling では学習時に重複したデータを学習して過学習を引き起こしたり、under-sampling では学習に重要なデータを適切にサンプリングして学習することが難しい。
そこで深層学習の文脈では損失関数に重み付けを行う &lt;code class=&quot;highlighter-rouge&quot;&gt;cost-sensitive learning&lt;/code&gt; を採用する場合が多いが、こうした手法は実世界の long-tail な分布を持つ不均衡データに対してパフォーマンスが低下してしまう場合が多い。
本研究では long-tail な不均衡データに対して、対象となるデータ数に効果的な &lt;code class=&quot;highlighter-rouge&quot;&gt;class-balanced loss&lt;/code&gt; を提案し、
一般的に広く使われている softmax cross-entropy や sigmoid cross-entropy、focal loss などに適用し効果の検証を行っている。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;h3 id=&quot;class-balanced-loss&quot;&gt;Class-balanced loss&lt;/h3&gt;

&lt;p&gt;各クラス数に反比例する重み係数を導入することによって、long-tail な不均衡データに対しても効率的に学習するよう損失関数を定義した。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\rm CB}({\bf p}, y) = \frac{1}{E_{n_y}} \mathcal{L}({\bf p}, y) = \frac{1 - \beta}{1 - \beta^{n_y}} \mathcal{L}({\bf p}, y)&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/cv/Class-Balanced-Loss-Based-on-Effective-Number-of-Samples/figure3.png&quot; width=&quot;600px&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Class-balanced loss を一般的な損失関数に適用した場合は以下の通りになる。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Class-balanced softmax cross-entropy
    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;{\rm CB}_{\rm softmax}({\bf z}, y) = - \frac{1 - \beta}{1 - \beta^{n_y}} \log{\left( \frac{\exp{(z_y)}}{\sum_{j=1}^{C} \exp{(z_j)}} \right)}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Class-balanced sigmoid cross-entropy
    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;{\rm CB}_{\rm sigmoid}({\bf z}, y) = - \frac{1 - \beta}{1 - \beta^{n_y}} \sum_{i=1}^{C} \log{\left(\frac{1}{1+\exp{(-z_{i}^{t})}} \right)}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Class-balanced focal loss
    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;{\rm CB}_{\rm focal}({\bf z}, y) = \frac{1 - \beta}{1 - \beta^{n_y}} \sum_{i=1}^{C} (1 - p_{i}^{t})^{\gamma} \log{(p_{i}^{t})}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;CIFAR10/100 に対して imbalanced factor を元にデータの分布を擬似的に不均衡にした &lt;code class=&quot;highlighter-rouge&quot;&gt;long-tailed CIFAR10/100&lt;/code&gt; 、&lt;code class=&quot;highlighter-rouge&quot;&gt;iNatiralist&lt;/code&gt; 、&lt;code class=&quot;highlighter-rouge&quot;&gt;ImageNet&lt;/code&gt; を用いて、ベースラインのモデルと class-balanced loss を導入したモデルの比較を行っている。&lt;/p&gt;

&lt;p&gt;Sigmoid ベースの loss を用いる場合は、最終全結合層のバイアスに対してクラスの事前確率を &lt;script type=&quot;math/tex&quot;&gt;\pi = 1/C&lt;/script&gt; として、&lt;script type=&quot;math/tex&quot;&gt;b = - \log{((1 - \pi)/{\pi})}&lt;/script&gt; として初期化し、バイアス項にのみ weight decay を適用している。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;p&gt;画像認識タスクでは主に softmax cross-entropy が用いられるが、バイアス項を適切に初期化した sigmoid cross-entropy や focal loss が softmax cross-entropy を凌駕する結果を示した。&lt;/p&gt;

&lt;p&gt;Class-balanced loss のハイパーパラーメタである &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; は CIFAR-10 の場合 &lt;script type=&quot;math/tex&quot;&gt;0.9999&lt;/script&gt; であったが、CIFAR-100 の場合は imbalanced factor ごとに異なる &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; の設定が必要であった。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;re-sampling-ベースの手法&quot;&gt;Re-sampling ベースの手法&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-319-46478-7_29&quot;&gt;Shen, Li, Zhouchen Lin, and Qingming Huang. “Relay backpropagation for effective learning of deep convolutional neural networks.” European conference on computer vision. Springer, Cham, 2016.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.00941&quot;&gt;Geifman, Yonatan, and Ran El-Yaniv. “Deep active learning over the long tail.” arXiv preprint arXiv:1711.00941 (2017).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0893608018302107&quot;&gt;Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. “A systematic study of the class imbalance problem in convolutional neural networks.” Neural Networks 106 (2018): 249-259.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.07911&quot;&gt;Zou, Yang, et al. “Domain adaptation for semantic segmentation via class-balanced self-training.” arXiv preprint arXiv:1810.07911 (2018).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.6858&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Drummond, Chris, and Robert C. Holte. “C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling.” Workshop on learning from imbalanced datasets II. Vol. 11. Washington, DC: Citeseer, 2003.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jair.org/papers/paper953.html&quot;&gt;Chawla, Nitesh V., et al. “SMOTE: synthetic minority over-sampling technique.” Journal of artificial intelligence research 16 (2002): 321-357.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cost-sensitive-learning-ベースの手法&quot;&gt;Cost-Sensitive Learning ベースの手法&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.8819&quot;&gt;Ting, Kai Ming. “A comparative study of cost-sensitive boosting algorithms.” In Proceedings of the 17th International Conference on Machine Learning. 2000.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.computer.org/csdl/trans/tk/2006/01/k0063-abs.html&quot;&gt;Zhou, Zhi-Hua, and Xu-Ying Liu. “Training cost-sensitive neural networks with methods addressing the class imbalance problem.” IEEE Transactions on Knowledge &amp;amp; Data Engineering 1 (2006): 63-77.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8012579/&quot;&gt;Khan, Salman H., et al. “Cost-sensitive learning of deep feature representations from imbalanced data.” IEEE transactions on neural networks and learning systems 29.8 (2018): 3573-3587.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ECCV_2018/html/Nikolaos_Sarafianos_Deep_Imbalanced_Attribute_ECCV_2018_paper.html&quot;&gt;Sarafianos, Nikolaos, Xiang Xu, and Ioannis A. Kakadiaris. “Deep imbalanced attribute classification using visual attention aggregation.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;importance-sampling&quot;&gt;Importance sampling&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pubsonline.informs.org/doi/abs/10.1287/opre.1.5.263&quot;&gt;Kahn, Herman, and Andy W. Marshall. “Methods of reducing sample size in Monte Carlo computations.” Journal of the Operations Research Society of America 1.5 (1953): 263-278.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;決定境界を一定に調整&quot;&gt;決定境界を一定に調整&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://cseweb.ucsd.edu/~elkan/rescale.pdf&quot;&gt;Elkan, Charles. “The foundations of cost-sensitive learning.” International joint conference on artificial intelligence. Vol. 17. No. 1. Lawrence Erlbaum Associates Ltd, 2001.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;クラス数の逆数で重み付け&quot;&gt;クラス数の逆数で重み付け&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7278-learning-to-model-the-tail&quot;&gt;Wang, Yu-Xiong, Deva Ramanan, and Martial Hebert. “Learning to model the tail.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Huang_Learning_Deep_Representation_CVPR_2016_paper.html&quot;&gt;Huang, Chen, et al. “Learning deep representation for imbalanced classification.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;クラス数の逆数の平方根で重み付け&quot;&gt;クラス数の逆数の平方根で重み付け&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases&quot;&gt;Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems. 2013.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ECCV_2018/html/Dhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper.html&quot;&gt;Mahajan, Dhruv, et al. “Exploring the limits of weakly supervised pretraining.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;難しいサンプルにフォーカスして学習&quot;&gt;難しいサンプルにフォーカスして学習&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S002200009791504X&quot;&gt;Freund, Yoav, and Robert E. Schapire. “A decision-theoretic generalization of on-line learning and an application to boosting.” Journal of computer and system sciences 55.1 (1997): 119-139.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.208.446&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Malisiewicz, Tomasz, Abhinav Gupta, and Alexei A. Efros. “Ensemble of exemplar-SVMs for object detection and beyond.” Iccv. Vol. 1. No. 2. 2011.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_iccv_2017/html/Dong_Class_Rectification_Hard_ICCV_2017_paper.html&quot;&gt;Dong, Qi, Shaogang Gong, and Xiatian Zhu. “Class rectification hard mining for imbalanced deep learning.” Proceedings of the IEEE International Conference on Computer Vision. 2017.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_iccv_2017/html/Lin_Focal_Loss_for_ICCV_2017_paper.html&quot;&gt;Lin, Tsung-Yi, et al. “Focal loss for dense object detection.” Proceedings of the IEEE international conference on computer vision. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;ノイジーなデータやラベルミスなデータにフォーカスして学習&quot;&gt;ノイジーなデータやラベルミスなデータにフォーカスして学習&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3305576&quot;&gt;Koh, Pang Wei, and Percy Liang. “Understanding black-box predictions via influence functions.” Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.09050&quot;&gt;Ren, Mengye, et al. “Learning to reweight examples for robust deep learning.” arXiv preprint arXiv:1803.09050 (2018).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.05555&quot;&gt;Cui, Yin, et al. “Class-Balanced Loss Based on Effective Number of Samples.” arXiv preprint arXiv:1901.05555 (2019).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Attention Convolutional Neural Network for Advertiser level Click through Rate Forecasting</title><link href="https://shunk031.github.io/paper-survey/summary/others/Attention-Convolutional-Neural-Network-for-Advertiser-level-Click-through-Rate-Forecasting" rel="alternate" type="text/html" title="Attention Convolutional Neural Network for Advertiser level Click through Rate Forecasting" /><published>2019-05-09T00:00:00+00:00</published><updated>2019-05-09T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Attention-Convolutional-Neural-Network-for-Advertiser-level-Click-through-Rate-Forecasting</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Attention-Convolutional-Neural-Network-for-Advertiser-level-Click-through-Rate-Forecasting">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;広告主単位&lt;/code&gt; という新たな視点で CTR 予測を行う Context-aware Attention Convolutional Neural Network を提案した。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;CTR 予測問題はオンライン広告における重要な問題の 1 つである。先行研究では &lt;code class=&quot;highlighter-rouge&quot;&gt;ユーザ単位&lt;/code&gt; の CTR 予測モデルの提案は複数行われているが、 &lt;code class=&quot;highlighter-rouge&quot;&gt;広告主単位&lt;/code&gt; での予測は本研究が著者が知りうる限りで初めてである。
通常集計されたログデータをモデルに入力するが、本研究ではログデータを時系列データとみなししてモデルに入力し学習を行う。&lt;/p&gt;

&lt;p&gt;本研究では、以下の特徴を持つ context-aware attention convolutional neural network (CACNN) を提案し、実世界のデータセットに対して CTR 予測を行った結果を報告している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Attention-Convolutional-Neural-Network-for-Advertiser-level-Click-through-Rate-Forecasting/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;context-aware-attention-convolutional-neural-network-cacnn&quot;&gt;Context-aware Attention Convolutional Neural Network (CACNN)&lt;/h3&gt;
&lt;p&gt;時系列の CTR データに対する Attention CNN とコンテキストデータ対する MLP からなるネットワーク構造を持つ。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;時系列の CTR データに対する Attention CNN
    &lt;ul&gt;
      &lt;li&gt;畳み込みで得られた特徴マップに対して attention weight を掛ける
        &lt;ul&gt;
          &lt;li&gt;局所的な非線形性や季節性の傾向を適切に捉えることが可能&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;コンテキストデータに対する MLP
    &lt;ul&gt;
      &lt;li&gt;本研究では配信対象の国とデバイスタイプを使用する
        &lt;ul&gt;
          &lt;li&gt;実際の CTR と国・デバイスタイプは相関関係にあるため、これらの情報を適切に捉えることが重要&lt;/li&gt;
          &lt;li&gt;通常こうした特徴量は同一空間上には存在しないが、複数の全結合層を経て concat することにより、同一の潜在空間で扱うことが可能&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;インターネット企業の主要な広告プラットフォームから 30 日分の広告キャンペーンデータ 20 万件程度を取得し、これらを元に評価を行っている。
ベースラインのモデルとして古典的な手法 (exponential moving average 系)と深層学習ベースの手法 (CNN, CNN with Attention 等)と提案手法である CACNN の比較を行っている。&lt;/p&gt;

&lt;h3 id=&quot;学習方法と評価方法について&quot;&gt;学習方法と評価方法について&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;train として 29 日分のデータを使用した。
    &lt;ul&gt;
      &lt;li&gt;28 日目までのデータを使って 29 日目を予測するよう学習を行う。&lt;/li&gt;
      &lt;li&gt;valid として train からランダムに 10%程度サンプリングして使用した。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;test として 30 日目のデータを使用した。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h3 id=&quot;畳み込み操作の効果の確認&quot;&gt;畳み込み操作の効果の確認&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Attention-Convolutional-Neural-Network-for-Advertiser-level-Click-through-Rate-Forecasting/figure12.png&quot; alt=&quot;Figure 12&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ランダムに時系列の CTR データをサンプリングし、それらに対する畳み込み後の特徴マップの状態を可視化したもの。
    &lt;ul&gt;
      &lt;li&gt;異なるカーネルサイズで異なる時系列の特徴を捉えていることが分かる。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;attention-の効果確認&quot;&gt;Attention の効果確認&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Figure 13 (a)&lt;/th&gt;
      &lt;th&gt;Figure 13 (b)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Attention-Convolutional-Neural-Network-for-Advertiser-level-Click-through-Rate-Forecasting/figure13a.png&quot; alt=&quot;Figure 13 (a)&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Attention-Convolutional-Neural-Network-for-Advertiser-level-Click-through-Rate-Forecasting/figure13b.png&quot; alt=&quot;Figure 13 (b)&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 13 (a) : ある特徴マップ 50 個を対象に、それらに対する attention の重みを可視化した図&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Figure 13 (b) : (a)の特徴マップ先頭 3 つを対象に、時系列データを入力したときの特徴マップ値の変化を可視化したもの。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Figure 13 (a) において、特徴マップ 1 と特徴マップ 3 は attention の重みが似ている。
    &lt;ul&gt;
      &lt;li&gt;Figure 13 (b) で特徴マップ 1 (赤) と 特徴マップ 3 (青) は時系列データに対して同様の反応をしている。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;特徴マップ 2 は、1 と 3 と同様の傾向の反応パターンを示しているが、元の時系列データ (黄) と大きさという点では離れている。
    &lt;ul&gt;
      &lt;li&gt;大きな誤差に対して学習された attention の重みが小さくなっているため、最終的な予測誤差は小さくなっていると考えられる。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;p&gt;特になし。&lt;/p&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3186184&quot;&gt;Gao, Hongchang, et al. “Attention Convolutional Neural Network for Advertiser-level Click-through Rate Forecasting.” Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2018.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Beyond News Contents: The Role of Social Context for Fake News Detection</title><link href="https://shunk031.github.io/paper-survey/summary/others/Beyond-News-Contents-The-Role-of-Social-Context-for-Fake-News-Detection" rel="alternate" type="text/html" title="Beyond News Contents: The Role of Social Context for Fake News Detection" /><published>2019-03-20T00:00:00+00:00</published><updated>2019-03-20T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/others/Beyond-News-Contents-The-Role-of-Social-Context-for-Fake-News-Detection</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/others/Beyond-News-Contents-The-Role-of-Social-Context-for-Fake-News-Detection">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;出版社・ニュース記事・ユーザの 3 つの関係をモデリングすることでフェイクニュース検出の精度向上を目指す TriFN を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;フェイクニュース検出は主にニュース記事にフォーカスしたものと、ユーザのソーシャルな行動にフォーカスしているものが多い。
これらは偽の情報を判断する言語的な観点やフェイク画像に対する視覚的観点に基づいたモデルの構築を行っている。
またユーザに対して行動ログベースやユーザ間のネットワークベースのモデリングを行っている事例も多い。&lt;/p&gt;

&lt;p&gt;本研究では、先行研究では考慮されてこなかった、&lt;code class=&quot;highlighter-rouge&quot;&gt;出版社&lt;/code&gt;・&lt;code class=&quot;highlighter-rouge&quot;&gt;ニュース記事&lt;/code&gt;・&lt;code class=&quot;highlighter-rouge&quot;&gt;ユーザ行動&lt;/code&gt;の 3 つの関係 &lt;code class=&quot;highlighter-rouge&quot;&gt;tri-relationship&lt;/code&gt; を考慮することでフェイクニュース検出の精度向上を目指す TriFN を提案している。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Beyond-News-Contents-The-Role-of-Social-Context-for-Fake-News-Detection/figure1.png&quot; width=&quot;400px&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/others/Beyond-News-Contents-The-Role-of-Social-Context-for-Fake-News-Detection/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;tri-relationship-を考慮したフェイクニュース検出モデル-trifn&quot;&gt;Tri-relationship を考慮したフェイクニュース検出モデル TriFN&lt;/h3&gt;
&lt;p&gt;TriFN は主に non-negative matrix factorization (NMF) をベースとした 5 つのコンポーネントから構成される。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;News Contents Embedding
    &lt;ul&gt;
      &lt;li&gt;Bag-of-words 表現なニュース記事から NMF でニュース記事の潜在表現を学習する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;User Embedding
    &lt;ul&gt;
      &lt;li&gt;ユーザのソーシャルな関係から NMF でユーザの潜在表現を学習する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;User-News Interaction Embedding
    &lt;ul&gt;
      &lt;li&gt;ユーザの信頼度によって得られるニュース記事の潜在表現を学習する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Publisher-News Relation Embedding
    &lt;ul&gt;
      &lt;li&gt;出版社の党派的なバイアスを考慮したニュース記事の潜在表現を学習する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Semi-supervised Linear Classifier
    &lt;ul&gt;
      &lt;li&gt;ニュース記事に対してフェイクニュース予測を行うための学習器を学習する&lt;/li&gt;
      &lt;li&gt;半教師あり学習の枠組みを同時に使用し、ラベルが付与されていないデータからも学習する&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;FakeNewsNet データセットを用いて、&lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt;か&lt;code class=&quot;highlighter-rouge&quot;&gt;fake&lt;/code&gt;かの 2 値分類問題としてフェイクニュース検出に対する検出精度の比較を行っている。
比較対象として先行研究で提案されている複数の特徴抽出手法 (RST, LIWC, Castillo とこれらを組み合わせた RST + Castillo, LIWC + Castillo) とベースラインのモデル (LogReg, NBayes, DTree, RForest, XGBoost, AdaBoost, Gradient Boosting) で比較している。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;早期のフェイクニュース検出におけるパフォーマンス
    &lt;ul&gt;
      &lt;li&gt;記事公開から 12 時間後 〜 96 時間後の各時間におけるフェイクニュース検出の精度比較を行った結果、提案手法の TriFN が一番良かった。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;モデルのパラメータ分析
    &lt;ul&gt;
      &lt;li&gt;ユーザのソーシャルな関係の寄与をコントロールするパラメータを大きくすると精度が向上した。
        &lt;ul&gt;
          &lt;li&gt;ユーザとニュース間の関係よりユーザのソーシャルな関係のほうが予測に寄与する。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;フェイクニュース検出における data augmentation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8594871/&quot;&gt;Shu, Kai, et al. “Deep headline generation for clickbait detection.” 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 2018.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1712.07709&quot;&gt;Shu, Kai, Suhang Wang, and Huan Liu. “Beyond News Contents: The Role of Social Context for Fake News Detection.” Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining. ACM, 2019.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">How Large Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/How-Large-Vocabulary-Does-Text-Classification-Need-A-Variational-Approach-to-Vocabulary-Selection" rel="alternate" type="text/html" title="How Large Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection" /><published>2019-03-02T00:00:00+00:00</published><updated>2019-03-02T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/How-Large-Vocabulary-Does-Text-Classification-Need-A-Variational-Approach-to-Vocabulary-Selection</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/How-Large-Vocabulary-Does-Text-Classification-Need-A-Variational-Approach-to-Vocabulary-Selection">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;精度を保ったまま最小限の語彙を選択する variational vocabulary dropout (VDD) を提案。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;自然言語処理 (NLP) のタスクに対して deep learning モデルを用いる場合、入力にあらかじめ定義された語彙を元に単語をベクトル化して入力する必要がある。
こうした語彙数は非常に大規模となる場合が多くパラメータ数も増大してしまうため、限られた計算リソース下での実行することは難しい。
したがってタスクを解く精度を保ったまま必要な語彙を選択する必要がある。&lt;/p&gt;

&lt;p&gt;本研究ではこの語彙選択問題に対して、タスクを考慮した語彙選択手法である variational vocabulary dropoput (VDD) を提案している。
また適切な語彙選択が行われているかを確認するため、AUC ベースの評価指標を導入し、提案手法の効果を確認している。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;
&lt;h3 id=&quot;語彙選択に対する問題設定&quot;&gt;語彙選択に対する問題設定&lt;/h3&gt;
&lt;p&gt;元の embedding &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;と語彙選択を行った後の embedding &lt;script type=&quot;math/tex&quot;&gt;\hat{W}&lt;/script&gt; を使用した場合に、予測精度の差が閾値以上である語彙数を最小にする。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathop{\rm argmin}\limits_{\hat{W},\hat{\theta}} \textrm{#Row}(\hat{W})~~s.t. \textrm{Acc}(f_{\hat{\theta}}(x; \hat{W}), y) - \textrm{Acc}(f_{\theta}(x;W),y)\le \epsilon&lt;/script&gt;

&lt;h3 id=&quot;語彙選択に対する評価指標&quot;&gt;語彙選択に対する評価指標&lt;/h3&gt;
&lt;p&gt;AUC ベースの評価指標 &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab@-X%&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;X ％のパフォーマンス低下が許される場合に必要な最小の語彙数を計算する
    &lt;ul&gt;
      &lt;li&gt;本研究では &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab@-3%&lt;/code&gt; および &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab@-5%&lt;/code&gt; を用いている&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;variational-vocabulary-dropout-vdd&quot;&gt;Variational Vocabulary Dropout (VDD)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Bernouli Dropout
    &lt;ul&gt;
      &lt;li&gt;Onehot ベクトルに対してベルヌーイノイズ&lt;script type=&quot;math/tex&quot;&gt;\textbf{b}&lt;/script&gt;を適用する
        &lt;ul&gt;
          &lt;li&gt;
            &lt;script type=&quot;math/tex; mode=display&quot;&gt;E(x|\textbf{b}) = (\textbf{b} \odot \textrm{OneHot}(x)) \cdot W&lt;/script&gt;
            &lt;ul&gt;
              &lt;li&gt;しかしながらベルヌーイ分布を reparameterize するのは難しい&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gaussian Relaxation
    &lt;ul&gt;
      &lt;li&gt;ベルヌーイノイズの代わりにガウシアンノイズ &lt;script type=&quot;math/tex&quot;&gt;z_i \sim \mathcal{N}(1, \alpha_{i} = \frac{p_i}{1 - p_i})&lt;/script&gt; を適用する
        &lt;ul&gt;
          &lt;li&gt;
            &lt;script type=&quot;math/tex; mode=display&quot;&gt;E(x|\textbf{z}) = (\textbf{z} \odot \textrm{OneHot}(x) \cdot W)&lt;/script&gt;
            &lt;ul&gt;
              &lt;li&gt;Reparameterization trick に従うと、&lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; は多変量ガウス分布 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; を用いて以下のようになる
                &lt;ul&gt;
                  &lt;li&gt;
                    &lt;script type=&quot;math/tex; mode=display&quot;&gt;E(x|\textbf{z}) = \textrm{OneHot}(x) \cdot B&lt;/script&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/How-Large-Vocabulary-Does-Text-Classification-Need-A-Variational-Approach-to-Vocabulary-Selection/figure2.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dropout 率 &lt;script type=&quot;math/tex&quot;&gt;\alpha_i&lt;/script&gt; は 語彙の&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の単語が必要かどうかを示す指標となる。
ここで&lt;script type=&quot;math/tex&quot;&gt;\alpha_i&lt;/script&gt;より数値が大きい場合は&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の語彙をドロップしてもパフォーマンス低下の要因にならないことを意味する。&lt;/p&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;文書分類 (document classification: DC)、自然言語理解 (natural language understanding: NLU)、自然言語推論 (natural language inference: NLI) のデータセットを用いてそれぞれのタスクにおける VVD の効果を確認している。
提案手法の VVD に対して、ベースラインとして頻度に基づいた語彙形成、TF-IDF に基づいた語彙形成、そして group lasso に基づいた語彙形成を比較している。
各タスクではそれぞれ DC では CNN ベースのモデル、NLU では attetion を導入した bi-directional LSTM モデル、NLI では ESIM モデルを用いている。&lt;/p&gt;

&lt;p&gt;すべてのタスクに対して提案手法の語彙選択手法である VVD が outperform する結果となっている。
また語彙選択を考慮した評価指標として Vocab@-X% を用いた場合では、特に提案手法のスコアが良くなっていることが示されており、効果的な語彙選択が行われていることがわかる。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;

&lt;h3 id=&quot;学習時の速度について&quot;&gt;学習時の速度について&lt;/h3&gt;
&lt;p&gt;VVD は確率的な振る舞いを扱うため、テキスト分類に対して学習を行う場合通常の cross entropy よりも時間がかかった。
フルサイズの語彙に対して VVD を適用すると計算時間がかかるため、前段階で精度低下が怒らない程度に語彙を制限したほうがよい。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Deep learning モデルに対するベイジアンベースのモデル圧縮について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6921-bayesian-compression-for-deep-learning&quot;&gt;Louizos, Christos, Karen Ullrich, and Max Welling. “Bayesian compression for deep learning.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.10339&quot;&gt;Wenhu Chen, Yu Su, Yilin Shen, Zhiyu Chen, Xifeng Yan, William Wang. How Large Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection. arXiv:1902.10339, 2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry><entry><title type="html">Attentional Encoder Network for Targeted Sentiment Classification</title><link href="https://shunk031.github.io/paper-survey/summary/nlp/Attentional-Encoder-Network-for-Targeted-Sentiment-Classification" rel="alternate" type="text/html" title="Attentional Encoder Network for Targeted Sentiment Classification" /><published>2019-03-01T00:00:00+00:00</published><updated>2019-03-01T00:00:00+00:00</updated><id>https://shunk031.github.io/paper-survey/summary/nlp/Attentional-Encoder-Network-for-Targeted-Sentiment-Classification</id><content type="html" xml:base="https://shunk031.github.io/paper-survey/summary/nlp/Attentional-Encoder-Network-for-Targeted-Sentiment-Classification">&lt;h2 id=&quot;1-どんなもの&quot;&gt;1. どんなもの？&lt;/h2&gt;

&lt;p&gt;ターゲットに対する感情分析で使われてきた LSTM に変わる self-attention ベースの attentional encoder network (AEN) を提案した。
また付与されている感情ラベルに対する非信頼性に対して label smoothing を行う損失関数を導入した。&lt;/p&gt;

&lt;h2 id=&quot;2-先行研究と比べてどこがすごいの&quot;&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;

&lt;p&gt;自然言語処理 (NLP) のタスクに対して RNN ベースの LSTM モデルが広く用いられてきたが、
コンテキスト内のターゲットに対する感情を分析する fine-grained なターゲットに対する感情分類 (targeted sentiment classification) に対しては依然として改善の余地が残されている。
先行研究の問題点としてはテキストのモデリングにおいて RNN ベースのモデルに比重がおかれており、また学習速度も遅いことが挙げられる。
感情分類タスクにおいて attention は重要な役割を果たすが、モデルの後段部分でのみしか使用されてこなかった。
また感情分類タスクにおいて付与されている感情ラベルは信頼性が低い場合がある。特に &lt;em&gt;neutral&lt;/em&gt; な感情ラベルはとてもファジーでモデルの学習を難しくしている。&lt;/p&gt;

&lt;p&gt;本研究では fine-grained なターゲットに対する感情分析にタスクに対してターゲットとコンテキストの交互作用を self-attention で捉え、ファジーな感情ラベルに対して効果的な label smoothing を行う損失関数を導入した。&lt;/p&gt;

&lt;h2 id=&quot;3-技術や手法のキモはどこにある&quot;&gt;3. 技術や手法の”キモ”はどこにある？&lt;/h2&gt;

&lt;p&gt;Attentional Encoder Network &lt;code class=&quot;highlighter-rouge&quot;&gt;(AEN)&lt;/code&gt; を提案&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Attentional-Encoder-Network-for-Targeted-Sentiment-Classification/figure1.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;attentional-encoder-layer&quot;&gt;Attentional Encoder Layer&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;LSTM に代わる並列計算可能なレイヤー
    &lt;ul&gt;
      &lt;li&gt;コンテキストとターゲットの embedding に対して隠れ状態の表現を計算&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Multi-Head Attention&lt;/code&gt; と &lt;code class=&quot;highlighter-rouge&quot;&gt;Point-wise Convolutional Transformation&lt;/code&gt; から構成される&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;multi-head-attention-mha&quot;&gt;Multi-Head Attention (MHA)&lt;/h4&gt;
&lt;p&gt;入力されるコンテキストとターゲットに対して異なる MHA を適用する。
MHA は RNN と比較して短いネットワークで離れた単語のコンテキストを捉えることが可能。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Intra-MHA
    &lt;ul&gt;
      &lt;li&gt;コンテキスト embedding &lt;script type=&quot;math/tex&quot;&gt;\textbf{e}^c&lt;/script&gt; を MHA に入力&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{e}^{\textrm{intra}} = MHA(\textbf{e}^c, \textbf{e}^c)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Inter-MHA
    &lt;ul&gt;
      &lt;li&gt;コンテキスト embedding &lt;script type=&quot;math/tex&quot;&gt;\textbf{e}^c&lt;/script&gt; と ターゲット embedding &lt;script type=&quot;math/tex&quot;&gt;\textbf{e}^t&lt;/script&gt; を MHA に入力&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{e}^{\textrm{inter}} = MHA(\textbf{e}^c, \textbf{e}^t)&lt;/script&gt;

&lt;h4 id=&quot;point-wise-convolution-transformation-pct&quot;&gt;Point-wise Convolution Transformation (PCT)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;2 つの 1D convolution と活性化関数から構成される&lt;/li&gt;
  &lt;li&gt;MHA で得られた表現に対して位置ごとにコンテキストを学習することが可能&lt;/li&gt;
  &lt;li&gt;Intra-MHA と inter-MHA それぞれに対して PCT を適用し、コンテキスト表現を得る&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;label-smoothing-regularization-lsr&quot;&gt;Label Smoothing Regularization (LSR)&lt;/h3&gt;

&lt;p&gt;感情分類タスクに対して &lt;em&gt;neutral&lt;/em&gt; というラベルはとてもファジーであるため、LSR を導入し、過学習を抑制している。
LSR は感情ラベルの分布とモデル予測の分布との KL 距離に等しいため、LSR 項は以下のように表せる。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}_{lsr} = - D_{\textrm{KL}} (u(k)||p_{\theta})&lt;/script&gt;

&lt;h2 id=&quot;4-どうやって有効だと検証した&quot;&gt;4. どうやって有効だと検証した？&lt;/h2&gt;

&lt;p&gt;SemEval 2014 Task4 (レストランのレビューデータセット、ラップトップのレビューデータセット)、ACL 14 Twitter データセットの 3 つで比較を行っている。
これらのデータセットには 3 つの感情ラベル (positive/neutral/negative) が付与されている。&lt;/p&gt;

&lt;p&gt;ベースラインのモデルとして SVM、Rec-NN、TD-LSTM、ATAE-LSTM、IAN、MemNet、RAM を用いて提案手法の AEN との比較を行っている。&lt;/p&gt;

&lt;h2 id=&quot;5-議論はあるか&quot;&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h3 id=&quot;label-smoothing-regularization-の効果&quot;&gt;Label smoothing regularization の効果&lt;/h3&gt;
&lt;p&gt;Label smoothing regularization を導入することで、導入していないモデルよりも良い精度であった。
信頼性の低いラベルに対して効果的に過学習を抑制できたと考えられる。&lt;/p&gt;

&lt;h3 id=&quot;recurrence-vsattention&quot;&gt;Recurrence vs.Attention&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/paper-survey/assets/img/nlp/Attentional-Encoder-Network-for-Targeted-Sentiment-Classification/table3.png&quot; width=&quot;400px&quot; alt=&quot;Table 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AEN に対して Attention encoder layer を LSTM に代えたネットワークと比較すると同等程度の精度を記録している。
よって LSTM に比べて半数程度の少ないパラメータで同程度の予測精度を達成できるメリットがある。&lt;/p&gt;

&lt;h2 id=&quot;6-次に読むべき論文はあるか&quot;&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-Head Atteition について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need&quot;&gt;Vaswani, Ashish, et al. “Attention is all you need.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Label Smoothing Regularization について
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html&quot;&gt;Szegedy, Christian, et al. “Rethinking the inception architecture for computer vision.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;論文情報リンク&quot;&gt;論文情報・リンク&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.09314&quot;&gt;Song, Youwei, et al. “Attentional Encoder Network for Targeted Sentiment Classification.” arXiv preprint arXiv:1902.09314 (2019).&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. どんなもの？</summary></entry></feed>